{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ddd1e7b",
   "metadata": {
    "executionInfo": {
     "elapsed": 21103,
     "status": "ok",
     "timestamp": 1746218890330,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "6ddd1e7b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras import layers, models, losses, metrics\n",
    "from tensorflow.keras import regularizers, callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LeakyReLU, Reshape, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Activation, Add, Input, GlobalAveragePooling1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vwlKQaX_p6I0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14669,
     "status": "ok",
     "timestamp": 1746218933338,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "vwlKQaX_p6I0",
    "outputId": "bd06a9a8-aed5-433c-d786-7e1d01b223e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf5715",
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1746218944545,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "1bbf5715"
   },
   "outputs": [],
   "source": [
    "def add_remaining_useful_life(df):\n",
    "    # Get the total number of cycles for each unit\n",
    "    grouped_by_unit = df.groupby(by=\"unit_nr\")\n",
    "    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n",
    "\n",
    "    # Merge the max cycle back into the original frame\n",
    "    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n",
    "\n",
    "    # Calculate remaining useful life for each row\n",
    "    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n",
    "    result_frame[\"RUL\"] = remaining_useful_life\n",
    "\n",
    "    # Drop max_cycle as it's no longer needed\n",
    "    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n",
    "    return result_frame\n",
    "\n",
    "def add_operating_condition(df):\n",
    "    df_op_cond = df.copy()\n",
    "\n",
    "    df_op_cond['setting_1'] = abs(df_op_cond['setting_1'].round())\n",
    "    df_op_cond['setting_2'] = abs(df_op_cond['setting_2'].round(decimals=2))\n",
    "\n",
    "    # Converting settings to string and concatanating makes the operating condition into a categorical variable\n",
    "    df_op_cond['op_cond'] = df_op_cond['setting_1'].astype(str) + '_' + \\\n",
    "                        df_op_cond['setting_2'].astype(str) + '_' + \\\n",
    "                        df_op_cond['setting_3'].astype(str)\n",
    "\n",
    "    return df_op_cond\n",
    "\n",
    "def condition_scaler(df_train, df_test, sensor_names):\n",
    "    # Apply operating condition specific scaling\n",
    "    scaler = StandardScaler()\n",
    "    for condition in df_train['op_cond'].unique():\n",
    "        scaler.fit(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_train.loc[df_train['op_cond']==condition, sensor_names] = scaler.transform(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_test.loc[df_test['op_cond']==condition, sensor_names] = scaler.transform(df_test.loc[df_test['op_cond']==condition, sensor_names])\n",
    "    return df_train, df_test\n",
    "\n",
    "def exponential_smoothing(df, sensors, n_samples, alpha=0.4):\n",
    "    df = df.copy()\n",
    "    # Take the exponential weighted mean\n",
    "    df[sensors] = df.groupby('unit_nr')[sensors].apply(lambda x: x.ewm(alpha=alpha).mean()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Drop first n_samples of each unit_nr to reduce filter delay\n",
    "    def create_mask(data, samples):\n",
    "        result = np.ones_like(data)\n",
    "        result[0:samples] = 0\n",
    "        return result\n",
    "\n",
    "    mask = df.groupby('unit_nr')['unit_nr'].transform(create_mask, samples=n_samples).astype(bool)\n",
    "    df = df[mask]\n",
    "\n",
    "    return df\n",
    "\n",
    "def gen_train_data(df, sequence_length, columns):\n",
    "    data = df[columns].values\n",
    "    num_elements = data.shape[0]\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements-(sequence_length-1)), range(sequence_length, num_elements+1)):\n",
    "        yield data[start:stop, :]\n",
    "\n",
    "def gen_data_wrapper(df, sequence_length, columns, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit_nr'].unique()\n",
    "\n",
    "    data_gen = (list(gen_train_data(df[df['unit_nr']==unit_nr], sequence_length, columns))\n",
    "               for unit_nr in unit_nrs)\n",
    "    data_array = np.concatenate(list(data_gen)).astype(np.float32)\n",
    "    return data_array\n",
    "\n",
    "def gen_labels(df, sequence_length, label):\n",
    "    data_matrix = df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    return data_matrix[sequence_length-1:num_elements, :]\n",
    "\n",
    "def gen_label_wrapper(df, sequence_length, label, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit_nr'].unique()\n",
    "\n",
    "    label_gen = [gen_labels(df[df['unit_nr']==unit_nr], sequence_length, label)\n",
    "                for unit_nr in unit_nrs]\n",
    "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "    return label_array\n",
    "\n",
    "def gen_test_data(df, sequence_length, columns, mask_value):\n",
    "    if df.shape[0] < sequence_length:\n",
    "        data_matrix = np.full(shape=(sequence_length, len(columns)), fill_value=mask_value)\n",
    "        idx = data_matrix.shape[0] - df.shape[0]\n",
    "        data_matrix[idx:,:] = df[columns].values\n",
    "    else:\n",
    "        data_matrix = df[columns].values\n",
    "\n",
    "    stop = data_matrix.shape[0]\n",
    "    start = stop - sequence_length\n",
    "    for i in list(range(1)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "def get_data(dataset, sensors, sequence_length, alpha, threshold):\n",
    "\tdir_path = './CMAPSS/'\n",
    "\ttrain_file = 'train_'+dataset+'.txt'\n",
    "\ttest_file = 'test_'+dataset+'.txt'\n",
    "\n",
    "\tindex_names = ['unit_nr', 'time_cycles']\n",
    "\tsetting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "\tsensor_names = ['s_{}'.format(i+1) for i in range(0,21)]\n",
    "\tcol_names = index_names + setting_names + sensor_names\n",
    "\n",
    "\ttrain = pd.read_csv((dir_path+train_file), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=col_names)\n",
    "\ttest = pd.read_csv((dir_path+test_file), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=col_names)\n",
    "\ty_test = pd.read_csv((dir_path+'RUL_'+dataset+'.txt'), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=['RemainingUsefulLife'])\n",
    "\n",
    "\ttrain = add_remaining_useful_life(train)\n",
    "\ttrain['RUL'].clip(upper=threshold, inplace=True)\n",
    "\n",
    "  #Dropping sensors\n",
    "\tdrop_sensors = [element for element in sensor_names if element not in sensors]\n",
    "\n",
    "  # Scale with respect to the operating condition\n",
    "\tX_train_pre = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "\tX_test_pre = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\tX_train_pre, X_test_pre = condition_scaler(X_train_pre, X_test_pre, sensors)\n",
    "\n",
    "  # Exponential smoothing\n",
    "\tX_train_pre= exponential_smoothing(X_train_pre, sensors, 0, alpha)\n",
    "\tX_test_pre = exponential_smoothing(X_test_pre, sensors, 0, alpha)\n",
    "\n",
    "  # Train/Validation split\n",
    "\tgss = GroupShuffleSplit(n_splits=1, train_size=0.80, random_state=42)\n",
    "\n",
    "\tfor train_unit, val_unit in gss.split(X_train_pre['unit_nr'].unique(), groups=X_train_pre['unit_nr'].unique()):\n",
    "\t\ttrain_unit = X_train_pre['unit_nr'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "\t\tval_unit = X_train_pre['unit_nr'].unique()[val_unit]\n",
    "\n",
    "\t\tx_train = gen_data_wrapper(X_train_pre, sequence_length, sensors, train_unit)\n",
    "\t\ty_train = gen_label_wrapper(X_train_pre, sequence_length, ['RUL'], train_unit)\n",
    "\n",
    "\t\tx_val = gen_data_wrapper(X_train_pre, sequence_length, sensors, val_unit)\n",
    "\t\ty_val = gen_label_wrapper(X_train_pre, sequence_length, ['RUL'], val_unit)\n",
    "\n",
    "\t# Create sequences for test\n",
    "\ttest_gen = (list(gen_test_data(X_test_pre[X_test_pre['unit_nr']==unit_nr], sequence_length, sensors, -99.))\n",
    "\t\t\t   for unit_nr in X_test_pre['unit_nr'].unique())\n",
    "\tx_test = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "\ttest_unit_ids = X_test_pre['unit_nr'].unique()\n",
    "\n",
    "\treturn x_train, y_train, x_val, y_val, x_test, y_test['RemainingUsefulLife'], test_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee519ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6229,
     "status": "ok",
     "timestamp": 1746218960034,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "3ee519ad",
    "outputId": "420c92c1-0ca2-4142-96f5-fb35725065db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-edafd160830c>:118: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train['RUL'].clip(upper=threshold, inplace=True)\n",
      "<ipython-input-3-edafd160830c>:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.1674041  -1.59188977 -0.1674041  ...  0.54483874  1.96932442\n",
      "  2.68156725]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_train.loc[df_train['op_cond']==condition, sensor_names] = scaler.transform(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
      "<ipython-input-3-edafd160830c>:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.87964693  1.25708158 -0.87964693 ... -0.1674041  -1.59188977\n",
      " -0.1674041 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_test.loc[df_test['op_cond']==condition, sensor_names] = scaler.transform(df_test.loc[df_test['op_cond']==condition, sensor_names])\n"
     ]
    }
   ],
   "source": [
    "# Choose the subset (FD001, FD002, FD003, FD004)\n",
    "dataset = 'FD002'\n",
    "\n",
    "# Sensors to use; sensor 13 is dropped from FD002 and FD004\n",
    "if(dataset == 'FD001' or 'FD003'):\n",
    "  sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "else:\n",
    "  sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "\n",
    "sequence_length = 30\n",
    "alpha = 0.3\n",
    "rul_clip_threshold = 125\n",
    "\n",
    "# Load and process the data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, test_unit_ids = get_data(\n",
    "    dataset=dataset,\n",
    "    sensors=sensors,\n",
    "    sequence_length=sequence_length,\n",
    "    alpha=alpha,\n",
    "    threshold=rul_clip_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba2f13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 772335,
     "status": "ok",
     "timestamp": 1745898425573,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "97ba2f13",
    "outputId": "3ceeedfd-f541-401b-8bb8-1097e636dc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 55ms/step - loss: 6817.8452 - root_mean_squared_error: 82.5035 - val_loss: 4461.7427 - val_root_mean_squared_error: 66.7963\n",
      "Epoch 2/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 53ms/step - loss: 4305.6074 - root_mean_squared_error: 65.5862 - val_loss: 2885.5969 - val_root_mean_squared_error: 53.7178\n",
      "Epoch 3/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 52ms/step - loss: 2795.3992 - root_mean_squared_error: 52.8506 - val_loss: 1843.4581 - val_root_mean_squared_error: 42.9355\n",
      "Epoch 4/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 1804.5582 - root_mean_squared_error: 42.4540 - val_loss: 1165.2659 - val_root_mean_squared_error: 34.1360\n",
      "Epoch 5/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - loss: 1122.4259 - root_mean_squared_error: 33.4884 - val_loss: 763.6387 - val_root_mean_squared_error: 27.6340\n",
      "Epoch 6/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - loss: 707.4575 - root_mean_squared_error: 26.5854 - val_loss: 488.1433 - val_root_mean_squared_error: 22.0940\n",
      "Epoch 7/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 455.3163 - root_mean_squared_error: 21.3313 - val_loss: 356.7177 - val_root_mean_squared_error: 18.8870\n",
      "Epoch 8/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 52ms/step - loss: 310.3208 - root_mean_squared_error: 17.6136 - val_loss: 296.8141 - val_root_mean_squared_error: 17.2283\n",
      "Epoch 9/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 240.7990 - root_mean_squared_error: 15.5156 - val_loss: 247.6060 - val_root_mean_squared_error: 15.7355\n",
      "Epoch 10/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 198.6463 - root_mean_squared_error: 14.0940 - val_loss: 248.6832 - val_root_mean_squared_error: 15.7697\n",
      "Epoch 11/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - loss: 180.4108 - root_mean_squared_error: 13.4312 - val_loss: 239.4899 - val_root_mean_squared_error: 15.4755\n",
      "Epoch 12/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - loss: 165.1767 - root_mean_squared_error: 12.8518 - val_loss: 232.3396 - val_root_mean_squared_error: 15.2427\n",
      "Epoch 13/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 154.1387 - root_mean_squared_error: 12.4150 - val_loss: 236.2685 - val_root_mean_squared_error: 15.3710\n",
      "Epoch 14/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - loss: 147.9185 - root_mean_squared_error: 12.1616 - val_loss: 281.4598 - val_root_mean_squared_error: 16.7768\n",
      "Epoch 15/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - loss: 140.1969 - root_mean_squared_error: 11.8388 - val_loss: 274.5879 - val_root_mean_squared_error: 16.5707\n",
      "Epoch 16/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 53ms/step - loss: 123.1437 - root_mean_squared_error: 11.0953 - val_loss: 267.6527 - val_root_mean_squared_error: 16.3601\n",
      "Epoch 17/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 53ms/step - loss: 120.9575 - root_mean_squared_error: 10.9971 - val_loss: 314.7298 - val_root_mean_squared_error: 17.7406\n",
      "Epoch 18/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 52ms/step - loss: 110.1293 - root_mean_squared_error: 10.4931 - val_loss: 328.4337 - val_root_mean_squared_error: 18.1227\n",
      "Epoch 19/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 52ms/step - loss: 102.8400 - root_mean_squared_error: 10.1388 - val_loss: 331.3166 - val_root_mean_squared_error: 18.2021\n",
      "Epoch 20/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 91.9623 - root_mean_squared_error: 9.5889 - val_loss: 326.7556 - val_root_mean_squared_error: 18.0764\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step\n",
      "Test RMSE: 29.32\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Masking(mask_value=0.0),\n",
    "        layers.LSTM(64, return_sequences=True),\n",
    "        layers.LSTM(32),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "rul_model = create_lstm_model(input_shape=(sequence_length, len(sensors)))\n",
    "\n",
    "history = rul_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    batch_size=64,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "predictions = rul_model.predict(x_test).flatten()\n",
    "rmse = np.sqrt(np.mean((predictions - y_test)**2))\n",
    "print(f\"Test RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4f4ea",
   "metadata": {
    "id": "ffb4f4ea"
   },
   "outputs": [],
   "source": [
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[2]\n",
    "        d_model_int = x.shape[-1]  # Static shape for slicing\n",
    "\n",
    "        position = tf.cast(tf.range(seq_len)[:, tf.newaxis], dtype=tf.float32)\n",
    "        div_term = tf.exp(\n",
    "            tf.cast(tf.range(0, d_model_int, 2), tf.float32) *\n",
    "            -(tf.math.log(10000.0) / tf.cast(d_model_int, tf.float32))\n",
    "        )\n",
    "        angle_rads = position * div_term\n",
    "\n",
    "        sines = tf.sin(angle_rads)\n",
    "        cosines = tf.cos(angle_rads)\n",
    "\n",
    "        # Interleave sines and cosines\n",
    "        pos_encoding = tf.reshape(\n",
    "            tf.stack([sines, cosines], axis=-1),\n",
    "            (seq_len, -1)\n",
    "        )\n",
    "\n",
    "        # Slice in case of dimension mismatch\n",
    "        pos_encoding = pos_encoding[:, :d_model_int]\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]  # (1, seq_len, d_model)\n",
    "        return x + pos_encoding\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "\n",
    "# Self-attention block\n",
    "def self_attention_block(x, num_heads=2, key_dim=32):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "# Final model\n",
    "def create_advanced_cnn_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Add positional encoding\n",
    "    x = PositionalEncodingLayer()(inputs)\n",
    "\n",
    "    # Dilated Convolutional Layers\n",
    "    x = layers.Conv1D(128, kernel_size=3, dilation_rate=1, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, kernel_size=3, dilation_rate=2, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(128, kernel_size=3, dilation_rate=4, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Self-Attention\n",
    "    x = self_attention_block(x)\n",
    "\n",
    "    # Global pooling + Dense layers\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=losses.Huber(),\n",
    "        metrics=[metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb700c",
   "metadata": {
    "id": "e7cb700c",
    "outputId": "3fb69803-e9ad-44fc-f722-088018b8375d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 60ms/step - loss: 84.2407 - root_mean_squared_error: 111.7238 - val_loss: 31.0359 - val_root_mean_squared_error: 38.0031\n",
      "Epoch 2/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 30.7822 - root_mean_squared_error: 39.1894 - val_loss: 20.6228 - val_root_mean_squared_error: 26.0661\n",
      "Epoch 3/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 23.1302 - root_mean_squared_error: 29.4339 - val_loss: 13.2701 - val_root_mean_squared_error: 19.0806\n",
      "Epoch 4/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 16.5938 - root_mean_squared_error: 21.9889 - val_loss: 12.4040 - val_root_mean_squared_error: 16.7813\n",
      "Epoch 5/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 14.9373 - root_mean_squared_error: 20.0033 - val_loss: 11.5782 - val_root_mean_squared_error: 17.2036\n",
      "Epoch 6/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 52ms/step - loss: 13.5753 - root_mean_squared_error: 18.4629 - val_loss: 13.5341 - val_root_mean_squared_error: 17.5894\n",
      "Epoch 7/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 52ms/step - loss: 13.5873 - root_mean_squared_error: 18.5502 - val_loss: 11.9730 - val_root_mean_squared_error: 15.6558\n",
      "Epoch 8/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 52ms/step - loss: 13.1943 - root_mean_squared_error: 18.0902 - val_loss: 11.1370 - val_root_mean_squared_error: 16.9262\n",
      "Epoch 9/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 52ms/step - loss: 13.3731 - root_mean_squared_error: 18.2535 - val_loss: 10.9112 - val_root_mean_squared_error: 16.5171\n",
      "Epoch 10/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 13.1646 - root_mean_squared_error: 17.9053 - val_loss: 11.8520 - val_root_mean_squared_error: 16.6057\n",
      "Epoch 11/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 13.2020 - root_mean_squared_error: 17.8896 - val_loss: 12.4969 - val_root_mean_squared_error: 16.7193\n",
      "Epoch 12/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 13.0175 - root_mean_squared_error: 17.7335 - val_loss: 11.5934 - val_root_mean_squared_error: 16.1764\n",
      "Epoch 13/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 64ms/step - loss: 13.0108 - root_mean_squared_error: 17.7474 - val_loss: 11.5155 - val_root_mean_squared_error: 17.3824\n",
      "Epoch 14/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 13.0799 - root_mean_squared_error: 17.7868 - val_loss: 10.6170 - val_root_mean_squared_error: 15.3672\n",
      "Epoch 15/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - loss: 12.6679 - root_mean_squared_error: 17.2931 - val_loss: 10.7101 - val_root_mean_squared_error: 15.5143\n",
      "Epoch 16/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - loss: 12.3736 - root_mean_squared_error: 16.9417 - val_loss: 10.4768 - val_root_mean_squared_error: 15.6748\n",
      "Epoch 17/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 12.3372 - root_mean_squared_error: 16.8239 - val_loss: 11.9437 - val_root_mean_squared_error: 16.1246\n",
      "Epoch 18/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 54ms/step - loss: 12.2560 - root_mean_squared_error: 16.7863 - val_loss: 12.4509 - val_root_mean_squared_error: 16.9146\n",
      "Epoch 19/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 55ms/step - loss: 11.9137 - root_mean_squared_error: 16.2893 - val_loss: 12.1051 - val_root_mean_squared_error: 16.6040\n",
      "Epoch 20/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 53ms/step - loss: 12.1112 - root_mean_squared_error: 16.4415 - val_loss: 13.1112 - val_root_mean_squared_error: 18.4128\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 83ms/step\n",
      "Test RMSE: 26.95\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_advanced_cnn_model(input_shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "predictions = cnn_model.predict(x_test).flatten()\n",
    "test_rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158a7a7",
   "metadata": {
    "id": "8158a7a7"
   },
   "outputs": [],
   "source": [
    "def residual_block(x, filters, kernel_size, dilation_rate=1):\n",
    "    shortcut = x\n",
    "    # If the input and output shapes are different, apply a 1x1 convolution to match the dimensions\n",
    "    if x.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "\n",
    "    # Convolution layers\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Add the residual (skip connection)\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def transformer_block(x, num_heads=2, key_dim=32):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    ff = layers.Dense(x.shape[-1] * 4, activation='relu')(x)\n",
    "    ff = layers.Dropout(0.2)(ff)\n",
    "    ff = layers.Dense(x.shape[-1])(ff)\n",
    "\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def create_cnn_transformer_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # CNN\n",
    "    x = residual_block(inputs, 64, kernel_size=3)\n",
    "    x = residual_block(x, 128, kernel_size=3, dilation_rate=2)\n",
    "    x = residual_block(x, 128, kernel_size=3, dilation_rate=4)\n",
    "\n",
    "    # Transformer\n",
    "    x = transformer_block(x, num_heads=4, key_dim=32)\n",
    "\n",
    "    # Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=losses.Huber(),\n",
    "        metrics=[metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085e351",
   "metadata": {
    "id": "8085e351",
    "outputId": "b4fb8861-7217-4211-fb3c-057ae40421dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 145ms/step - loss: 30.7924 - root_mean_squared_error: 41.4033 - val_loss: 12.4341 - val_root_mean_squared_error: 17.0099\n",
      "Epoch 2/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 134ms/step - loss: 13.0117 - root_mean_squared_error: 17.9854 - val_loss: 10.3699 - val_root_mean_squared_error: 16.5411\n",
      "Epoch 3/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 131ms/step - loss: 11.9797 - root_mean_squared_error: 16.8088 - val_loss: 11.0178 - val_root_mean_squared_error: 16.8527\n",
      "Epoch 4/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 132ms/step - loss: 11.4345 - root_mean_squared_error: 16.1369 - val_loss: 11.8163 - val_root_mean_squared_error: 18.1172\n",
      "Epoch 5/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 128ms/step - loss: 10.9938 - root_mean_squared_error: 15.4612 - val_loss: 10.3930 - val_root_mean_squared_error: 16.5231\n",
      "Epoch 6/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 130ms/step - loss: 10.5262 - root_mean_squared_error: 14.9669 - val_loss: 11.1824 - val_root_mean_squared_error: 16.8076\n",
      "Epoch 7/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 129ms/step - loss: 10.3454 - root_mean_squared_error: 14.7375 - val_loss: 12.4279 - val_root_mean_squared_error: 18.6979\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 159ms/step\n",
      "Test RMSE: 26.89\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_transformer_model(input_shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "predictions = cnn_model.predict(x_test).flatten()\n",
    "test_rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "print(f\"Test RMSE: {test_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ef860",
   "metadata": {
    "id": "b00ef860"
   },
   "outputs": [],
   "source": [
    "# Temporal Convolutional Block (with residual connections)\n",
    "def tcn_block(x, filters, kernel_size, dilation_rate):\n",
    "    shortcut = x\n",
    "    # If the number of filters in the input doesn't match the output, apply a 1x1 convolution\n",
    "    if x.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding='same')(x)\n",
    "\n",
    "    # Apply the dilated convolution\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Residual connection\n",
    "    x = layers.Add()([shortcut, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# TCN Model (without Time2Vec)\n",
    "def create_tcn_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Apply TCN Blocks\n",
    "    x = tcn_block(inputs, 64, kernel_size=3, dilation_rate=1)\n",
    "    x = tcn_block(x, 128, kernel_size=3, dilation_rate=2)\n",
    "    x = tcn_block(x, 128, kernel_size=3, dilation_rate=4)\n",
    "\n",
    "    # Global pooling and output layers\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss=losses.Huber(),\n",
    "        metrics=[metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9d8a7f",
   "metadata": {
    "id": "bc9d8a7f",
    "outputId": "281f9e7d-a8a4-4f49-af24-756d1de1647c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 77ms/step - loss: 26.0872 - root_mean_squared_error: 35.6328 - val_loss: 13.0142 - val_root_mean_squared_error: 18.8114\n",
      "Epoch 2/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - loss: 13.2673 - root_mean_squared_error: 18.6690 - val_loss: 12.7163 - val_root_mean_squared_error: 19.2424\n",
      "Epoch 3/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - loss: 11.5683 - root_mean_squared_error: 16.4082 - val_loss: 14.1488 - val_root_mean_squared_error: 21.3448\n",
      "Epoch 4/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 68ms/step - loss: 10.8874 - root_mean_squared_error: 15.5387 - val_loss: 12.6933 - val_root_mean_squared_error: 19.0816\n",
      "Epoch 5/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 70ms/step - loss: 10.1450 - root_mean_squared_error: 14.4437 - val_loss: 12.9155 - val_root_mean_squared_error: 18.9751\n",
      "Epoch 6/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 71ms/step - loss: 9.7754 - root_mean_squared_error: 13.9187 - val_loss: 12.8359 - val_root_mean_squared_error: 19.7613\n",
      "Epoch 7/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 72ms/step - loss: 9.5570 - root_mean_squared_error: 13.5151 - val_loss: 13.4450 - val_root_mean_squared_error: 19.9080\n",
      "Epoch 8/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - loss: 9.0356 - root_mean_squared_error: 12.8896 - val_loss: 13.1755 - val_root_mean_squared_error: 20.0461\n",
      "Epoch 9/20\n",
      "\u001b[1m585/585\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 71ms/step - loss: 9.2548 - root_mean_squared_error: 13.0742 - val_loss: 14.0533 - val_root_mean_squared_error: 21.1037\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step\n",
      "Test RMSE: 66.92311202910413\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_tcn_model(input_shape=(x_train.shape[1], x_train.shape[2]))\n",
    "\n",
    "# Training the model\n",
    "history = cnn_model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Prediction\n",
    "predictions = cnn_model.predict(x_test).flatten()\n",
    "test_rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
