{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ddd1e7b",
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1746218465785,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "6ddd1e7b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras import layers, models, losses, metrics\n",
    "from tensorflow.keras import regularizers, callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LeakyReLU, Reshape, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization, Conv2D, MaxPooling2D, Flatten, Activation, Add, Input, GlobalAveragePooling1D, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "vwlKQaX_p6I0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14654,
     "status": "ok",
     "timestamp": 1746218490115,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "vwlKQaX_p6I0",
    "outputId": "7e7c8400-9e9a-4ddf-c20b-258283f5e8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf5715",
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1746218468183,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "1bbf5715"
   },
   "outputs": [],
   "source": [
    "def add_remaining_useful_life(df):\n",
    "    # Get the total number of cycles for each unit\n",
    "    grouped_by_unit = df.groupby(by=\"unit_nr\")\n",
    "    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n",
    "\n",
    "    # Merge the max cycle back into the original frame\n",
    "    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n",
    "\n",
    "    # Calculate remaining useful life for each row\n",
    "    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n",
    "    result_frame[\"RUL\"] = remaining_useful_life\n",
    "\n",
    "    # Drop max_cycle as it's no longer needed\n",
    "    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n",
    "    return result_frame\n",
    "\n",
    "def add_operating_condition(df):\n",
    "    df_op_cond = df.copy()\n",
    "\n",
    "    df_op_cond['setting_1'] = abs(df_op_cond['setting_1'].round())\n",
    "    df_op_cond['setting_2'] = abs(df_op_cond['setting_2'].round(decimals=2))\n",
    "\n",
    "    # Converting settings to string and concatanating makes the operating condition into a categorical variable\n",
    "    df_op_cond['op_cond'] = df_op_cond['setting_1'].astype(str) + '_' + \\\n",
    "                        df_op_cond['setting_2'].astype(str) + '_' + \\\n",
    "                        df_op_cond['setting_3'].astype(str)\n",
    "\n",
    "    return df_op_cond\n",
    "\n",
    "def condition_scaler(df_train, df_test, sensor_names):\n",
    "    # Apply operating condition specific scaling\n",
    "    scaler = StandardScaler()\n",
    "    for condition in df_train['op_cond'].unique():\n",
    "        scaler.fit(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_train.loc[df_train['op_cond']==condition, sensor_names] = scaler.transform(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
    "        df_test.loc[df_test['op_cond']==condition, sensor_names] = scaler.transform(df_test.loc[df_test['op_cond']==condition, sensor_names])\n",
    "    return df_train, df_test\n",
    "\n",
    "def exponential_smoothing(df, sensors, n_samples, alpha=0.4):\n",
    "    df = df.copy()\n",
    "    # Take the exponential weighted mean\n",
    "    df[sensors] = df.groupby('unit_nr')[sensors].apply(lambda x: x.ewm(alpha=alpha).mean()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # Drop first n_samples of each unit_nr to reduce filter delay\n",
    "    def create_mask(data, samples):\n",
    "        result = np.ones_like(data)\n",
    "        result[0:samples] = 0\n",
    "        return result\n",
    "\n",
    "    mask = df.groupby('unit_nr')['unit_nr'].transform(create_mask, samples=n_samples).astype(bool)\n",
    "    df = df[mask]\n",
    "\n",
    "    return df\n",
    "\n",
    "def gen_train_data(df, sequence_length, columns):\n",
    "    data = df[columns].values\n",
    "    num_elements = data.shape[0]\n",
    "\n",
    "    for start, stop in zip(range(0, num_elements-(sequence_length-1)), range(sequence_length, num_elements+1)):\n",
    "        yield data[start:stop, :]\n",
    "\n",
    "def gen_data_wrapper(df, sequence_length, columns, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit_nr'].unique()\n",
    "\n",
    "    data_gen = (list(gen_train_data(df[df['unit_nr']==unit_nr], sequence_length, columns))\n",
    "               for unit_nr in unit_nrs)\n",
    "    data_array = np.concatenate(list(data_gen)).astype(np.float32)\n",
    "    return data_array\n",
    "\n",
    "def gen_labels(df, sequence_length, label):\n",
    "    data_matrix = df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    return data_matrix[sequence_length-1:num_elements, :]\n",
    "\n",
    "def gen_label_wrapper(df, sequence_length, label, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit_nr'].unique()\n",
    "\n",
    "    label_gen = [gen_labels(df[df['unit_nr']==unit_nr], sequence_length, label)\n",
    "                for unit_nr in unit_nrs]\n",
    "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "    return label_array\n",
    "\n",
    "def gen_test_data(df, sequence_length, columns, mask_value):\n",
    "    if df.shape[0] < sequence_length:\n",
    "        data_matrix = np.full(shape=(sequence_length, len(columns)), fill_value=mask_value)\n",
    "        idx = data_matrix.shape[0] - df.shape[0]\n",
    "        data_matrix[idx:,:] = df[columns].values\n",
    "    else:\n",
    "        data_matrix = df[columns].values\n",
    "\n",
    "    stop = data_matrix.shape[0]\n",
    "    start = stop - sequence_length\n",
    "    for i in list(range(1)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "def get_data(dataset, sensors, sequence_length, alpha, threshold):\n",
    "\tdir_path = './CMAPSS/'\n",
    "\ttrain_file = 'train_'+dataset+'.txt'\n",
    "\ttest_file = 'test_'+dataset+'.txt'\n",
    "\n",
    "\tindex_names = ['unit_nr', 'time_cycles']\n",
    "\tsetting_names = ['setting_1', 'setting_2', 'setting_3']\n",
    "\tsensor_names = ['s_{}'.format(i+1) for i in range(0,21)]\n",
    "\tcol_names = index_names + setting_names + sensor_names\n",
    "\n",
    "\ttrain = pd.read_csv((dir_path+train_file), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=col_names)\n",
    "\ttest = pd.read_csv((dir_path+test_file), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=col_names)\n",
    "\ty_test = pd.read_csv((dir_path+'RUL_'+dataset+'.txt'), sep=r'\\s+', header=None,\n",
    "\t\t\t\t\t names=['RemainingUsefulLife'])\n",
    "\n",
    "\ttrain = add_remaining_useful_life(train)\n",
    "\ttrain['RUL'].clip(upper=threshold, inplace=True)\n",
    "\n",
    "  #Dropping sensors\n",
    "\tdrop_sensors = [element for element in sensor_names if element not in sensors]\n",
    "\n",
    "  # Scale with respect to the operating condition\n",
    "\tX_train_pre = add_operating_condition(train.drop(drop_sensors, axis=1))\n",
    "\tX_test_pre = add_operating_condition(test.drop(drop_sensors, axis=1))\n",
    "\tX_train_pre, X_test_pre = condition_scaler(X_train_pre, X_test_pre, sensors)\n",
    "\n",
    "  # Exponential smoothing\n",
    "\tX_train_pre= exponential_smoothing(X_train_pre, sensors, 0, alpha)\n",
    "\tX_test_pre = exponential_smoothing(X_test_pre, sensors, 0, alpha)\n",
    "\n",
    "  # Train/Validation split\n",
    "\tgss = GroupShuffleSplit(n_splits=1, train_size=0.80, random_state=42)\n",
    "\n",
    "\tfor train_unit, val_unit in gss.split(X_train_pre['unit_nr'].unique(), groups=X_train_pre['unit_nr'].unique()):\n",
    "\t\ttrain_unit = X_train_pre['unit_nr'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "\t\tval_unit = X_train_pre['unit_nr'].unique()[val_unit]\n",
    "\n",
    "\t\tx_train = gen_data_wrapper(X_train_pre, sequence_length, sensors, train_unit)\n",
    "\t\ty_train = gen_label_wrapper(X_train_pre, sequence_length, ['RUL'], train_unit)\n",
    "\n",
    "\t\tx_val = gen_data_wrapper(X_train_pre, sequence_length, sensors, val_unit)\n",
    "\t\ty_val = gen_label_wrapper(X_train_pre, sequence_length, ['RUL'], val_unit)\n",
    "\n",
    "\t# Create sequences for test\n",
    "\ttest_gen = (list(gen_test_data(X_test_pre[X_test_pre['unit_nr']==unit_nr], sequence_length, sensors, -99.))\n",
    "\t\t\t   for unit_nr in X_test_pre['unit_nr'].unique())\n",
    "\tx_test = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "\ttest_unit_ids = X_test_pre['unit_nr'].unique()\n",
    "\n",
    "\treturn x_train, y_train, x_val, y_val, x_test, y_test['RemainingUsefulLife'], test_unit_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee519ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5101,
     "status": "ok",
     "timestamp": 1746218502342,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "3ee519ad",
    "outputId": "eb8e5af8-4ffb-463f-992d-3d3e3fab78dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-edafd160830c>:118: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train['RUL'].clip(upper=threshold, inplace=True)\n",
      "<ipython-input-4-edafd160830c>:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.1674041  -1.59188977 -0.1674041  ...  0.54483874  1.96932442\n",
      "  2.68156725]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_train.loc[df_train['op_cond']==condition, sensor_names] = scaler.transform(df_train.loc[df_train['op_cond']==condition, sensor_names])\n",
      "<ipython-input-4-edafd160830c>:36: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[-0.87964693  1.25708158 -0.87964693 ... -0.1674041  -1.59188977\n",
      " -0.1674041 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_test.loc[df_test['op_cond']==condition, sensor_names] = scaler.transform(df_test.loc[df_test['op_cond']==condition, sensor_names])\n"
     ]
    }
   ],
   "source": [
    "# Choose the subset (FD001, FD002, FD003, FD004)\n",
    "dataset = 'FD002'\n",
    "\n",
    "# Sensors to use; sensor 13 is dropped from FD002 and FD004\n",
    "if(dataset == 'FD001' or 'FD003'):\n",
    "  sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "else:\n",
    "  sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11', 's_12', 's_14', 's_15', 's_17', 's_20', 's_21']\n",
    "\n",
    "sequence_length = 30\n",
    "alpha = 0.3\n",
    "rul_clip_threshold = 125\n",
    "\n",
    "# Load and process the data\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, test_unit_ids = get_data(\n",
    "    dataset=dataset,\n",
    "    sensors=sensors,\n",
    "    sequence_length=sequence_length,\n",
    "    alpha=alpha,\n",
    "    threshold=rul_clip_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "t4xZdjTNvDxF",
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1746218537638,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "t4xZdjTNvDxF"
   },
   "outputs": [],
   "source": [
    "# Positional Encoding\n",
    "class PositionalEncodingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PositionalEncodingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[2]\n",
    "        pos = tf.range(seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        i = tf.range(d_model, dtype=tf.float32)[tf.newaxis, :]\n",
    "        angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = pos * angle_rates\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        return x + pos_encoding[tf.newaxis, :, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncodingLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "# CBAM Block\n",
    "def cbam_block(x, reduction_ratio=8):\n",
    "    input_channels = x.shape[-1]\n",
    "\n",
    "    # Channel Attention\n",
    "    avg_pool = layers.GlobalAveragePooling1D()(x)\n",
    "    max_pool = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    shared_dense_1 = layers.Dense(input_channels // reduction_ratio, activation='relu')\n",
    "    shared_dense_2 = layers.Dense(input_channels)\n",
    "\n",
    "    avg_out = shared_dense_2(shared_dense_1(avg_pool))\n",
    "    max_out = shared_dense_2(shared_dense_1(max_pool))\n",
    "\n",
    "    channel_attention = layers.Add()([avg_out, max_out])\n",
    "    channel_attention = layers.Activation('sigmoid')(channel_attention)\n",
    "    channel_attention = layers.Reshape((1, input_channels))(channel_attention)\n",
    "\n",
    "    x = layers.Multiply()([x, channel_attention])\n",
    "\n",
    "    # Spatial Attention\n",
    "    avg_pool_spatial = layers.Lambda(lambda z: tf.reduce_mean(z, axis=-1, keepdims=True))(x)\n",
    "    max_pool_spatial = layers.Lambda(lambda z: tf.reduce_max(z, axis=-1, keepdims=True))(x)\n",
    "    concat = layers.Concatenate(axis=-1)([avg_pool_spatial, max_pool_spatial])\n",
    "    spatial_attention = layers.Conv1D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "\n",
    "    x = layers.Multiply()([x, spatial_attention])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Residual Dilated CNN Block\n",
    "def residual_dilated_conv_block(x, filters, kernel_size, dilation_rate):\n",
    "    shortcut = x\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='same', dilation_rate=dilation_rate)(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Add()([shortcut, x]) if shortcut.shape[-1] == x.shape[-1] else layers.Conv1D(filters, 1)(shortcut) + x\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Transformer Block (MultiHead Attention)\n",
    "def transformer_block(x, num_heads=4, ff_dim=128):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(x, x)\n",
    "    attn_output = layers.Dropout(0.3)(attn_output)\n",
    "    out1 = layers.LayerNormalization()(x + attn_output)\n",
    "    ffn = layers.Dense(ff_dim, activation='relu')(out1)\n",
    "    ffn = layers.Dense(x.shape[-1])(ffn)\n",
    "    ffn = layers.Dropout(0.3)(ffn)\n",
    "    return layers.LayerNormalization()(out1 + ffn)\n",
    "\n",
    "# Model Definition\n",
    "def create_bilstm_cnn_hybrid_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = PositionalEncodingLayer()(inputs)\n",
    "\n",
    "    # BiLSTM Stack\n",
    "    x = layers.Bidirectional(layers.LSTM(96, return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(48, return_sequences=True))(x)\n",
    "\n",
    "    # Transformer Block\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    # CNN Blocks\n",
    "    x = residual_dilated_conv_block(x, filters=64, kernel_size=5, dilation_rate=1)\n",
    "    x = residual_dilated_conv_block(x, filters=32, kernel_size=5, dilation_rate=2)\n",
    "    x = cbam_block(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation='relu',\n",
    "                     kernel_regularizer=regularizers.l2(1e-5))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "LxuqhLv9dQhd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29099,
     "status": "ok",
     "timestamp": 1746218429540,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "LxuqhLv9dQhd",
    "outputId": "5d2f258b-353b-4b8e-95ac-e9d8e169fa29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/129.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-tuner\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V3Mma0F3e7Xz",
   "metadata": {
    "id": "V3Mma0F3e7Xz"
   },
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = PositionalEncodingLayer()(inputs)\n",
    "\n",
    "    # BiLSTM Stack\n",
    "    x = layers.Bidirectional(layers.LSTM(hp.Int('lstm_units1', 64, 128, step=32), return_sequences=True))(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(hp.Int('lstm_units2', 32, 64, step=16), return_sequences=True))(x)\n",
    "\n",
    "    # Transformer Block\n",
    "    x = transformer_block(\n",
    "        x,\n",
    "        num_heads=hp.Choice('num_heads', [2, 4, 8]),\n",
    "        ff_dim=hp.Choice('ff_dim', [64, 128, 256])\n",
    "    )\n",
    "\n",
    "    # Residual Dilated Convolutions\n",
    "    x = residual_dilated_conv_block(\n",
    "        x,\n",
    "        filters=hp.Choice('res_filters', [32, 64, 128]),\n",
    "        kernel_size=hp.Choice('kernel_size', [3, 5]),\n",
    "        dilation_rate=1\n",
    "    )\n",
    "    x = residual_dilated_conv_block(\n",
    "        x,\n",
    "        filters=hp.Choice('res_filters_2', [32, 64, 128]),\n",
    "        kernel_size=hp.Choice('kernel_size_2', [3, 5]),\n",
    "        dilation_rate=2\n",
    "    )\n",
    "\n",
    "    # CBAM\n",
    "    x = cbam_block(x)\n",
    "\n",
    "    # Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(\n",
    "        hp.Choice('dense_units', [32, 64, 128]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=regularizers.l2(1e-5)\n",
    "    )(x)\n",
    "    x = layers.Dropout(hp.Float('dropout', 0.2, 0.5, step=0.1))(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.AdamW(\n",
    "            learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='log'),\n",
    "            weight_decay=hp.Choice('weight_decay', [1e-6, 1e-5, 1e-4])\n",
    "        ),\n",
    "        loss=tf.keras.losses.Huber(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PqH26RH-fAxr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4434325,
     "status": "ok",
     "timestamp": 1745631915785,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "PqH26RH-fAxr",
    "outputId": "2288a960-ddb9-4ebb-b1d1-86ef6d273c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 03m 31s]\n",
      "val_root_mean_squared_error: 18.657495498657227\n",
      "\n",
      "Best val_root_mean_squared_error So Far: 14.860320091247559\n",
      "Total elapsed time: 01h 13m 48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adamw', because it has 2 variables whereas the saved optimizer has 118 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 97ms/step\n",
      "Test RMSE: 25.3737\n",
      "Test MAE: 17.2746\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_root_mean_squared_error',\n",
    "    max_trials=20,\n",
    "    directory='tuner_results',\n",
    "    project_name='bilstm_cnn_transformer_cbam'\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = best_model.predict(x_test).flatten()\n",
    "\n",
    "# Evaluate metrics\n",
    "rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
    "mae = np.mean(np.abs(predictions - y_test))\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Ji-gltwzoVD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1745632679781,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "-Ji-gltwzoVD",
    "outputId": "74121c73-bdfa-4e27-cee9-49b441285f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "lstm_units1: 96\n",
      "lstm_units2: 48\n",
      "num_heads: 4\n",
      "ff_dim: 128\n",
      "res_filters: 64\n",
      "kernel_size: 5\n",
      "res_filters_2: 32\n",
      "kernel_size_2: 5\n",
      "dense_units: 64\n",
      "dropout: 0.30000000000000004\n",
      "lr: 0.0005668473081877774\n",
      "weight_decay: 1e-05\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "for param in best_hps.values:\n",
    "    print(f\"{param}: {best_hps.get(param)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZQi7Qb-D5Tbo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1745634061712,
     "user": {
      "displayName": "Rajat Rayaraddi",
      "userId": "00873543174859166039"
     },
     "user_tz": 240
    },
    "id": "ZQi7Qb-D5Tbo",
    "outputId": "d4596e01-5098-4e94-ab45-5bb3a9975714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all trial results to keras_tuner_trials.csv\n"
     ]
    }
   ],
   "source": [
    "all_trials = tuner.oracle.trials.values()\n",
    "\n",
    "# Build a list of dictionaries with hyperparameters and scores\n",
    "results = []\n",
    "for trial in all_trials:\n",
    "    trial_data = trial.hyperparameters.values.copy()\n",
    "    trial_data['score'] = trial.score  # Typically validation loss or whatever your objective is\n",
    "    results.append(trial_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"keras_tuner_trials.csv\", index=False)\n",
    "print(\"Saved all trial results to keras_tuner_trials.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
